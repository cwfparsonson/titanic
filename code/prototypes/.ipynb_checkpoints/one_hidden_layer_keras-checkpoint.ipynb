{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple NN Using Keras API\n",
    "\n",
    "Before trying to dive into building NN from first-principles with TensorFlow, will first implement a simple NN making heavy use of the Keras API.\n",
    "\n",
    "This NN will have 1 input layer, 1 hidden layer, and 1 output layer. It will learn to predict whether or not a given passenger survived on the Titanic. Since there are only two possible outputs, this is a *binary classification* problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~*~*~* TensorFlow Version 2.1.0 *~*~*~\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "print('~*~*~* TensorFlow Version {} *~*~*~'.format(tf.__version__))\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Load data that we've already pre-processed for ML training & testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train: (891, 14)\n",
      "y train: (891, 1)\n"
     ]
    }
   ],
   "source": [
    "# load\n",
    "files_dir = '../../files/'\n",
    "df = pd.read_csv(files_dir+'processed_data.csv')\n",
    "\n",
    "# split into test and train depending on if has 'Surivived' label\n",
    "x_train = df[pd.notnull(df['Survived'])].drop(['Survived'], axis=1)\n",
    "y_train = df[pd.notnull(df['Survived'])]['Survived']\n",
    "x_test = df[pd.isnull(df['Survived'])].drop(['Survived'], axis=1)\n",
    "\n",
    "# reshape labels to be tensors (to match dims of logits) and extract values from dataframe\n",
    "y_train = y_train.iloc[:].values.reshape(-1,1)\n",
    "x_train = x_train.iloc[:].values\n",
    "\n",
    "print('x train: {}'.format(x_train.shape))\n",
    "print('y train: {}'.format(y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build NN\n",
    "\n",
    "The Keras API makes building NN models very simple by stacking sequential layers. For our simple model, we only want 3 stacked modules:\n",
    "\n",
    "1. **Input layer:** Flattens the data for the rest of the NN. Will have same number of neurons/units as there are dims in the input data.\n",
    "\n",
    "2. **Hidden layer:** Hidden layers are composed of 2 parts: (i) A linear layer and (ii) a non-linear activation function layer. Can use any number of nodes for the hidden layer. Keras will automatically initialise a matrix of weights, will matrix multiply these weights by the input data, and will add some bias vector to this multiplied matrix. This linear operation will give the data to input to the non-linear activation function (many diff types of activation func).\n",
    "\n",
    "3. **Output layer:** The output layer takes the outputs of the previous layer & performs a linear operation by matrix multiplying by some set of weights and adding biases. The output of this process is a vector whose elements are known as 'logits'. Logits are raw (non-normalised) 'predictions' generated by the classification NN model. The magnitude of each logit represents how 'confident' the NN is that the class represented by the particular logit is the true class of the data that was input into the model. Therefore the class with the highest value logit is the chosen classification of the NN. In our binary classification case, there are only 2 categories (0 = died, survived = 1), therefore only need 1 neuron/unit in output layer.\n",
    "\n",
    "After the output layer, the logits of the output vector are usually normalsied (such that the sum of the logits is 1) using e.g. a softmax func to give a probability distribution over all possible classes. In our binary classification case, only have 2 possible values, therefore we use the signmoid function to get an output between 0 and 1. Later on during testing, we will then round this to 0 or 1 to get the final answer of the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(x_train.shape[1],)),\n",
    "        tf.keras.layers.Dense(units=20, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=1, activation=tf.nn.sigmoid)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "As an example, we can use the first datum in our training data and our initialised NN model to get the probability that the NN thinks the datum is each possible class. N.B. in our above model, if we had not specified an activation in the last output layer, then the output model would be the model's non-normalised prediction/logit vector; since we have included a sigmoid activation function, this logit vector has been converted into a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First datum:\n",
      "[[-0.58162831 -0.4449995   0.84191642  1.          0.48128777  0.\n",
      "   0.          1.          1.          0.          0.          0.\n",
      "   0.          0.        ]]\n",
      "WARNING:tensorflow:Layer flatten is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Initialised model output probability that passenger survived: [[0.73537254]]\n"
     ]
    }
   ],
   "source": [
    "print('First datum:\\n{}'.format(x_train[:1]))\n",
    "predictions = model(x_train[:1]).numpy()\n",
    "print('Initialised model output probability that passenger survived: {}'.format(predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss Function\n",
    "\n",
    "Every NN needs a loss func for update rule/optimiser/backprop to use. This loss is the negative log probability of the true class: If it returns 0, the model is sure of the correct class.\n",
    "\n",
    "So far, we have only initialised our model; we have not trained it. Therefore our model's weights and biases will not have been optimised, and the untrained model will give close to random probabilities for each class being true were we to test this model.\n",
    "\n",
    "A common loss function for binary classification problems is the binary cross-entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model & Train\n",
    "\n",
    "We are now ready to compile our model by setting the optimiser, loss function, and the metric(s) to keep track of, and train our NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 891 samples\n",
      "Epoch 1/10\n",
      "891/891 [==============================] - 3s 3ms/sample - loss: 0.7242 - accuracy: 0.6004\n",
      "Epoch 2/10\n",
      "891/891 [==============================] - 2s 3ms/sample - loss: 0.6689 - accuracy: 0.6857\n",
      "Epoch 3/10\n",
      "891/891 [==============================] - 3s 3ms/sample - loss: 0.6516 - accuracy: 0.7475\n",
      "Epoch 4/10\n",
      "891/891 [==============================] - 3s 3ms/sample - loss: 0.6408 - accuracy: 0.7744\n",
      "Epoch 5/10\n",
      "891/891 [==============================] - 3s 3ms/sample - loss: 0.6356 - accuracy: 0.7980\n",
      "Epoch 6/10\n",
      "891/891 [==============================] - 3s 3ms/sample - loss: 0.6313 - accuracy: 0.8025\n",
      "Epoch 7/10\n",
      "891/891 [==============================] - 3s 3ms/sample - loss: 0.6288 - accuracy: 0.8182\n",
      "Epoch 8/10\n",
      "891/891 [==============================] - 3s 3ms/sample - loss: 0.6269 - accuracy: 0.8215\n",
      "Epoch 9/10\n",
      "891/891 [==============================] - 3s 3ms/sample - loss: 0.6256 - accuracy: 0.8249\n",
      "Epoch 10/10\n",
      "891/891 [==============================] - 3s 3ms/sample - loss: 0.6243 - accuracy: 0.8260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efe443cbe10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile nn\n",
    "model.compile(optimizer='adam',\n",
    "                  loss=loss_fn,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# train nn\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model\n",
    "\n",
    "With our model now trained, we can test it on the unseen (and unlabelled) test data. To get firm 0-1 predictions for our binary classifier, we round the probabilities output by the classifier to 0 or 1. Since this is for a Kaggle competition, we save the predictions/solution generated by our trained NN to a CSV file in the required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(files_dir+'test.csv')\n",
    "test['Survived'] = model.predict(x_test.iloc[:].values)\n",
    "test['Survived'] = test['Survived'].apply(lambda x: round(x, 0)).astype('int')\n",
    "solution = test[['PassengerId', 'Survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained nn solution for Kaggle entry\n",
    "solution.to_csv(files_dir+'nn_solution.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
